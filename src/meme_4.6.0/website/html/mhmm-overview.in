<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<meta name="generator" content=
"HTML Tidy for Linux/x86 (vers 1st January 2004), see www.w3.org">
<title>Meta-MEME v3.3</title>
<link href="metameme.css" rel="styleSheet" type="text/css">
</head>
<body bgcolor="white">
<blockquote>
<h1><img height="81" width="444" src="images/meta-meme.gif"></h1>
<h3>Overview</h3>
<hr>
<h3>Introduction</h3>
Meta-MEME is a software toolkit for building and using motif-based
hidden Markov models of biological sequences. The input to
Meta-MEME is a set of similar DNA or protein sequences, as well as
a set of motif models discovered by <a href=
"http://meme.sdsc.edu">MEME</a>. Meta-MEME combines these models
into a single, motif-based hidden Markov model and uses this model 
<!--to produce a multiple alignment of the original set of sequences
and--> to search a sequence database for homologs.
<h3>Program inputs</h3>
Meta-MEME takes as input two files:
<ul>
<li>The <b>sequence file</b> contains a set of similar DNA or
protein sequences that you are interested in modeling. Typically,
these will be sequences that are homologous to one another or that
share homologous domains. The sequences can be in various <a href=
"metameme-help-format.html">formats</a>.</li>
<li>The <b>MEME motif models</b> are based upon the sequence file.
A MEME motif model is a position-specific scoring matrix; i.e., a
matrix in which position (i,j) contains the probability that
residue i will appear in position j in the motif. MEME motifs are
gapless, so the model contains no gap opening or extension
penalties. MEME motif models can be generated by the <a href=
"http://meme.sdsc.edu/meme/website/cgi-bin/meme.cgi">MEME web server</a>
at the San Diego Supercomputer Center. Meta-MEME requires the MEME model
in HTML format.</li>
</ul>
<h3>Step one: Building a model</h3>
During the first, model-building step, Meta-MEME takes the
single-motif models from the given MEME file and combines them into
a motif-based hidden Markov model (HMM). An HMM is a generalization
of a finite state machine in which each state in the model
corresponds to a single residue. Probability distributions at each
state represent the probabilities of point mutations; transition
probabilities among states allow for insertions, deletions and, for
some model topologies, repeated or shuffled domains.
<p>Meta-MEME's hidden Markov models differ from standard HMMs such
as the ones produced by <a href=
"http://www.cse.ucsc.edu/research/compbio/sam.html">SAM</a> and
<a href="http://hmmer.wustl.edu">HMMER</a> in that Meta-MEME's
models are motif-based. Thus, the non-motif spacer regions are
modelled imprecisely, thereby significantly reducing the number of
parameters that need to be learned. This reduction in parameter
space allows Meta-MEME models to be accurately trained from smaller
sets of sequences.</p>
<p>In constructing the HMM, Meta-MEME uses information about the
order and spacing of motifs within the family. 
By default, Meta-MEME builds a model
with a linear topology, in which the motifs are arranged like beads
on a string.  
It is also possible to request that Meta-MEME build a model in
which every motif is connected to every other motif. This
completely connected topology allows for the accurate modeling of
families containing repeated or shuffled elements. <!--
<H3>Step two: Training the model</H3>

When the model is constructed, the transition probabilities between
motifs are initialized in an informed manner using transition
information from the MAST analysis.  However, it is often helpful to
train those transition probabilities via expectation-maximization so
as to maximize the posterior probability of the model, given the data.
This training is carried out automatically by the Meta-MEME server,
using as training data the sequences provided.  Many training options
are available.  They are described in more detail <A
HREF="mhmm-options.html">here</A>.

<H3>Step three: Multiple alignment</H3>

The trained model can be used to create a multiple alignment of the
given set of sequences.  Meta-MEME aligns a set of sequences by
aligning each individual sequence with the HMM via the Viterbi
algorithm.  This algorithm finds the series of model states that is
most likely to have generated a given sequence.  The result is an
alignment of states in the model to positions in the sequence.  A
multiple alignment is just a generalization of this sequence-to-model
alignment, in which all the positions that align with a particular
model state are aligned with one another.  Note that multiple
alignments, as they are commonly used, are intrinsically linear, so a
Meta-MEME model with a completely connected topology cannot be used to
create a multiple alignment.

<P>

The multiple alignment created by Meta-MEME is motif-based.  Thus,
only the motif regions of the given sequences are aligned.
Optionally, you may request that the non-motif regions be included in
the alignment but note that those regions will not be aligned with one
another.  A motif-only alignment may be usefully used as input to a
phylogenetic inference algorithm.
--></p>
<h3>Step two: Homology detection</h3>
<!--In addition to aligning the given sequences,-->A Meta-MEME
model can be used to search a sequence database for homologs. The
homology detection algorithm assigns to each sequence in the
database a score that is proportional to the probability that the
sequence was generated by the given model. Meta-MEME can compute
two types of scores: the Viterbi score is the probability
associated with the single path through the model that is most
likely to have generated the given sequence; the total probability
score is the sum of the probabilities of all possible paths through
the model. By default, Meta-MEME computes the Viterbi score, since
this score is less computationally expensive to compute. It is an
open question whether Viterbi or total probability scoring produces
better homology detection performance. You may request either or
both types of scores.
<p>Both Viterbi and total probability scores are reported as
log-odds scores in bits. An odds score is the ratio of the score of
the sequence with respect to the foreground model versus the score
of the sequence with respect to the background model. The log-odds
score is the log (in base 2) of this ratio. In Meta-MEME, the
foreground model is a motif-based HMM, and the background model is
a simple linear HMM that roughly captures the features of a typical
sequence. If the family in question is small relative to the size
of the database being searched, then the odds score is
approximately equivalent to the likelihood that the sequence
belongs to the family in question divided by the likelihood that it
does not. Hence, an odds score of 1 (or a log-odds score of 0)
implies equal likelihood that the sequence is a family member or is
not.</p>
<p>The threshold for statistical significance of an odds score
depends upon the expected number of family members in the database.
A sequence can be safely deemed a family member if its odds score
is greater than the ratio of the total number of sequences in the
database over the expected number of family members. For example,
if you are searching a database of 36000 sequences for a family
containing approximately 100 sequences, then any sequence that
receives an odds score of (36000 / 100) = 360 or higher likely
belongs to the family. Since Meta-MEME reports log-odds scores,
this threshold corresponds to log2(360) = 8.5.</p>
<p>For more information, read the Meta-MEME <a href=
"./doc/metameme.html">software documentation</a>.</p>
<hr>
<a href="metameme-intro.html">Return</a> to the Meta-MEME home page.
<p><i>Please send comments and questions to:
<a href=
"mailto:@METAMEME_CONTACT@">@METAMEME_CONTACT@</a>.</i></p>
</blockquote>
</body>
</html>
